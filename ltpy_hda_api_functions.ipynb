{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/EU-Copernicus-EUM_3Logos.png' alt='Logo EU Copernicus EUMETSAT' align='right' width='50%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonized Data Access API - Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook lists all `functions` that are defined and used to access data from `WEkEO` with the HDA API. The function were developed by **Ben Loveday** and **Hayley Evers-King** and improve the verbosity of the HDA API.\n",
    "\n",
    "The following functions are available:\n",
    "- [init](#init)\n",
    "- [get_access_token](#get_token)\n",
    "- [query_metadata](#query_metadata)\n",
    "- [accept_TandC](#accept_tc)\n",
    "- [launch_query](#launch_query)\n",
    "- [check_job_status](#job_status)\n",
    "- [get_results_list](#results_list)\n",
    "- [get_download_links](#get_download_links)\n",
    "- [downloadFile](#download_file)\n",
    "- [get_filename_from_cd](#get_filename_from_cd)\n",
    "- [get_filenames](#get_filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, json, urllib3, sys\n",
    "import shutil\n",
    "import time, os\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='init'></a> `init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(dataset_id, api_key, download_dir_path, verbose=True):\n",
    "    '''\n",
    "     Initisalise Harmonised Data Access API path dictionary\n",
    "    '''\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    HAPI_dict = {}\n",
    "    # HDA-API endpoint\n",
    "    HAPI_dict[\"apis_endpoint\"]=\"https://apis.wekeo.eu\"\n",
    "    # Data broker address\n",
    "    HAPI_dict[\"broker_address\"] = HAPI_dict[\"apis_endpoint\"]\\\n",
    "                                  + \"/databroker/0.1.0\"\n",
    "    # Terms and conditions\n",
    "    HAPI_dict[\"acceptTandC_address\"]\\\n",
    "            = HAPI_dict[\"apis_endpoint\"]\\\n",
    "            + \"/dcsi-tac/0.1.0/termsaccepted/Copernicus_General_License\"\n",
    "    # Access-token address\n",
    "    HAPI_dict[\"accessToken_address\"] = HAPI_dict[\"apis_endpoint\"]\\\n",
    "                                       + '/token'\n",
    "    # Dataset id\n",
    "    HAPI_dict[\"dataset_id\"] = dataset_id\n",
    "    HAPI_dict[\"encoded_dataset_id\"] = urllib.parse.quote(dataset_id)\n",
    "    # API key\n",
    "    HAPI_dict[\"api_key\"] = api_key\n",
    "\n",
    "    # set HTTP success code\n",
    "    HAPI_dict[\"CONST_HTTP_SUCCESS_CODE\"] = 200\n",
    "\n",
    "    # download directory\n",
    "    HAPI_dict[\"download_dir_path\"] = download_dir_path\n",
    "    if not os.path.exists(download_dir_path):\n",
    "        os.makedirs(download_dir_path)\n",
    "\n",
    "    # set verbosity\n",
    "    HAPI_dict[\"verbose\"] = verbose\n",
    "\n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='get_token'></a> `get_access_token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(HAPI_dict):\n",
    "    '''\n",
    "     Get an access token from an API key\n",
    "    '''\n",
    "    headers = {'Authorization': 'Basic ' + HAPI_dict[\"api_key\"]}\n",
    "    data = [('grant_type', 'client_credentials'), ]\n",
    "    print(\"Getting an access token. This token is valid for one hour only.\")\n",
    "    response = requests.post(HAPI_dict[\"accessToken_address\"],\\\n",
    "                headers=headers, data=data, verify=False)\n",
    "\n",
    "    # If the HTTP response code is 200 (i.e. success), then retrive the token from the response\n",
    "    if (response.status_code == HAPI_dict[\"CONST_HTTP_SUCCESS_CODE\"]):\n",
    "        access_token = json.loads(response.text)['access_token']\n",
    "        print(\"Success: Access token is \" + access_token)\n",
    "    else:\n",
    "        print(\"Error: Unexpected response {}\".format(response))\n",
    "\n",
    "    HAPI_dict[\"access_token\"] = access_token\n",
    "\n",
    "    HAPI_dict[\"headers\"] = \\\n",
    "       {'Authorization': 'Bearer ' + HAPI_dict[\"access_token\"],}\n",
    "\n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='query_metadata'></a> `query_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_metadata(HAPI_dict):\n",
    "    '''\n",
    "     Check data product metadata\n",
    "    '''\n",
    "    response = requests.get(HAPI_dict[\"broker_address\"] + '/querymetadata/'\\\n",
    "               + HAPI_dict[\"encoded_dataset_id\"],\\\n",
    "               headers=HAPI_dict[\"headers\"])\n",
    "    print('Getting query metadata, URL Is '\\\n",
    "          + HAPI_dict[\"broker_address\"] + '/querymetadata/'\\\n",
    "          + HAPI_dict[\"encoded_dataset_id\"] + \"?access_token=\"\\\n",
    "          + HAPI_dict[\"access_token\"])\n",
    "    print(\"************** Query Metadata for \" + HAPI_dict[\"dataset_id\"]\\\n",
    "         +\" **************\")\n",
    "\n",
    "    if (response.status_code == HAPI_dict[\"CONST_HTTP_SUCCESS_CODE\"]):\n",
    "        parsedResponse = json.loads(response.text)\n",
    "        print(json.dumps(parsedResponse, indent=4, sort_keys=True))\n",
    "        print(\"*****************************************\"\\\n",
    "                  + \"*********************************\")\n",
    "    else:\n",
    "        print(\"Error: Unexpected response {}\".format(response))\n",
    "        parsedResponse = \"Error\"\n",
    "\n",
    "    HAPI_dict[\"parsedResponse\"] = parsedResponse\n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='accept_tc'></a> `accept_TandC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_TandC(HAPI_dict):\n",
    "    '''\n",
    "     Accept terms a conditions (could replace with tick-box wiki, but not automated)\n",
    "    '''\n",
    "    response = requests.get(HAPI_dict[\"acceptTandC_address\"],\\\n",
    "               headers=HAPI_dict[\"headers\"])\n",
    "    isTandCAccepted = json.loads(response.text)['accepted']\n",
    "    if isTandCAccepted is False:\n",
    "        print(\"Accepting Terms and Conditions of Copernicus_General_License\")\n",
    "        response = requests.put(acceptTandC_address, headers=headers)\n",
    "    else:\n",
    "        print(\"Copernicus_General_License Terms and Conditions already accepted\")\n",
    "\n",
    "    HAPI_dict[\"isTandCaccepted\"] = True\n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='launch_query'></a> `launch_query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_query(HAPI_dict, query):\n",
    "    '''\n",
    "     Launch query and get job ID\n",
    "    '''\n",
    "    response = requests.post(HAPI_dict[\"broker_address\"] + '/datarequest',\\\n",
    "               headers=HAPI_dict[\"headers\"],\\\n",
    "               json=query, verify=False)\n",
    "    if (response.status_code == HAPI_dict[\"CONST_HTTP_SUCCESS_CODE\"]):\n",
    "        job_id = json.loads(response.text)['jobId']\n",
    "        print (\"Query successfully submitted. Job ID is \" + job_id)\n",
    "    else:\n",
    "        print(\"Error: Unexpected response {}\".format(response))\n",
    "        job_id = None\n",
    "\n",
    "    HAPI_dict[\"job_id\"] = job_id\n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='job_status'></a> `check_job_status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_job_status(HAPI_dict):\n",
    "    '''\n",
    "     Check status of HAPI job\n",
    "    '''\n",
    "    isComplete = False\n",
    "    while not isComplete:\n",
    "        response = requests.get(HAPI_dict[\"broker_address\"]\\\n",
    "                   + '/datarequest/status/' + HAPI_dict[\"job_id\"],\\\n",
    "                   headers=HAPI_dict[\"headers\"])\n",
    "        results = json.loads(response.text)['resultNumber']\n",
    "        isComplete = json.loads(response.text)['complete']\n",
    "        if isComplete:\n",
    "            print(\"The Job \" + HAPI_dict[\"job_id\"]\\\n",
    "              + \" has completed\")\n",
    "        else:\n",
    "           print(\"The Job \" + HAPI_dict[\"job_id\"]\\\n",
    "              + \" has not completed\")\n",
    "           # sleep for 2 seconds before checking the job status again\n",
    "           time.sleep(2)\n",
    "\n",
    "    numberOfResults = str(results)\n",
    "    HAPI_dict['nresults'] = results\n",
    "    print (\"Total number of products/results: \" + numberOfResults)\n",
    "    \n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='results_list'></a> `get_results_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_list(HAPI_dict):\n",
    "    '''\n",
    "     Return query results\n",
    "    '''\n",
    "    response = requests.get(HAPI_dict[\"broker_address\"]\\\n",
    "               + '/datarequest/jobs/' + HAPI_dict[\"job_id\"] + '/result',\\\n",
    "               headers=HAPI_dict[\"headers\"])\n",
    "    results = json.loads(response.text)\n",
    "\n",
    "    if HAPI_dict[\"verbose\"] == True:\n",
    "        print(\"************** Results *******************************\")\n",
    "        print(json.dumps(results, indent=4, sort_keys=True))\n",
    "        print(\"*********************************************\")\n",
    "\n",
    "    HAPI_dict[\"results\"] = results\n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='get_download_links'></a> `get_download_links`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_download_links(HAPI_dict):\n",
    "    '''\n",
    "     Get download links associated with results\n",
    "    '''\n",
    "    download_urls = []\n",
    "    for result in HAPI_dict[\"results\"]['content']:\n",
    "        externalUri = result['externalUri']\n",
    "        product_size = result['fileSize']/(1024*1024)\n",
    "        product_name = result['fileName']\n",
    "        download_url = HAPI_dict[\"broker_address\"]\\\n",
    "                       + '/datarequest/result/' + HAPI_dict[\"job_id\"]\\\n",
    "                       + '?externalUri='\\\n",
    "                       + urllib.parse.quote(externalUri)\\\n",
    "                       +\"&access_token=\"+HAPI_dict[\"access_token\"]\n",
    "        if HAPI_dict[\"verbose\"] == True:\n",
    "            print(\"Download link for \" + product_name\\\n",
    "                  + \"(\" + \"{:.2f}\".format(product_size) + \" MB) :\")\n",
    "            print(download_url)\n",
    "        download_urls.append(download_url)\n",
    "\n",
    "    HAPI_dict[\"download_urls\"] = download_urls\n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='download_file'></a> `downloadFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFile(url, headers, directory, total_length = 0) :\n",
    "    \"\"\"\n",
    "    downloadFile(url, headers, directory)\n",
    "    Download the file using streaming\n",
    "    Parameters\n",
    "    ----------\n",
    "    url:\n",
    "        The URL for the request\n",
    "    headers:\n",
    "        Headers (e.g. auth bearer token)\n",
    "    directory:\n",
    "        Directory for the download\n",
    "    size:\n",
    "        The size of the file\n",
    "    \"\"\"\n",
    "    r = requests.get(url, headers=headers, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        filename = os.path.join(directory,\\\n",
    "                   get_filename_from_cd(r.headers.get('content-disposition')))\n",
    "        print(\"Downloading \" + filename)\n",
    "        with open(filename, 'wb') as f:\n",
    "            start = time.clock()\n",
    "            print(\"File size is: %8.2f MB\" % (total_length/(1024*1024)))\n",
    "            dl = 0\n",
    "            for chunk in r.iter_content(64738):\n",
    "                dl += len(chunk)\n",
    "                f.write(chunk)\n",
    "                if total_length is not None:\n",
    "                    done = int(50 * dl / total_length)\n",
    "                    str1 = '=' * done\n",
    "                    str2 =' ' * (50-done)\n",
    "                    str3 = (dl//(time.clock() - start))/(1024*1024)\n",
    "                    print(\"\\r[%s%s] %8.2f Mbps\" % (str1, str2, str3), end='', flush=True)\n",
    "                else:\n",
    "                    if( dl % (1024) == 0 ):\n",
    "                        str1 = dl / (1024 * 1024)\n",
    "                        str2 = (dl//(time.clock() - start))/1024\n",
    "                        print(\"[%8.2f] MB downloaded, %8.2f kbps\" % (str1, str2))\n",
    "            str1 = dl / (1024 * 1024)\n",
    "            str2 = (dl//(time.clock() - start))/1024\n",
    "            print(\"[%8.2f] MB downloaded, %8.2f kbps\" % (str1, str2))\n",
    "            return (time.clock() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='get_filename_from_cd'></a> `get_filename_from_cd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_cd(cd):\n",
    "    \"\"\"\n",
    "    get_filename_from_cd(cd)\n",
    "    Get the filename from content disposition\n",
    "    Parameters\n",
    "    ----------\n",
    "    cd : content disposition\n",
    "        From https://www.w3.org/Protocols/rfc2616/rfc2616-sec19.html\n",
    "        The Content-Disposition response-header field has been proposed \n",
    "        as a means for the origin server to suggest a default filename\n",
    "        if the user requests that the content is saved to a file.\n",
    "        This usage is derived from the definition of Content-Disposition in RFC 1806 [35].\n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        return None\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    return fname[0].replace(\"'\",\"\").replace('\"',\"\")\n",
    "\n",
    "def download_data(HAPI_dict, skip_existing=False, verbose=False): \n",
    "    '''\n",
    "     Download the data\n",
    "    '''\n",
    "    counter = 0\n",
    "    filenames = []\n",
    "    for result in HAPI_dict[\"results\"]['content']:\n",
    "        externalUri = result['externalUri']\n",
    "        product_size = result['fileSize']\n",
    "        download_url = HAPI_dict[\"broker_address\"]\\\n",
    "                       + '/datarequest/result/'\\\n",
    "                       + HAPI_dict[\"job_id\"] + '?externalUri='\\\n",
    "                       + urllib.parse.quote(externalUri)\n",
    "        \n",
    "        r = requests.get(download_url, headers=HAPI_dict[\"headers\"],\\\n",
    "                         stream=True)\n",
    "\n",
    "        if verbose:\n",
    "            print(r)\n",
    "            print(r.headers)\n",
    "            print(r.headers.get('content-disposition'))\n",
    "            \n",
    "        filename = os.path.join(HAPI_dict[\"download_dir_path\"],\\\n",
    "                   get_filename_from_cd(r.headers.get('content-disposition')))\n",
    "        filenames.append(filename)\n",
    "        \n",
    "        if skip_existing and os.path.exists(filename):\n",
    "            print(\"Skipping \" + os.path.basename(filename) + \" as it exists already\")\n",
    "        else:\n",
    "            time_elapsed = downloadFile(download_url, HAPI_dict[\"headers\"],\\\n",
    "                          HAPI_dict[\"download_dir_path\"], product_size)\n",
    "            print(\"Download complete (took \" + str(time_elapsed) + \" seconds)\")\n",
    "            print(\"\")\n",
    "            \n",
    "    HAPI_dict['filenames'] = filenames\n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='get_filenames'></a> `get_filenames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(HAPI_dict):\n",
    "    '''\n",
    "     Get the filenames of the target file\n",
    "    '''\n",
    "    filenames = []\n",
    "    for result in HAPI_dict[\"results\"]['content']:\n",
    "        externalUri = result['externalUri']\n",
    "        product_size = result['fileSize']\n",
    "        download_url = HAPI_dict[\"broker_address\"]\\\n",
    "                       + '/datarequest/result/'\\\n",
    "                       + HAPI_dict[\"job_id\"] + '?externalUri='\\\n",
    "                       + urllib.parse.quote(externalUri)\n",
    "\n",
    "        r = requests.get(download_url, headers=HAPI_dict[\"headers\"],\\\n",
    "                         stream=True)\n",
    "        filename = os.path.join(HAPI_dict[\"download_dir_path\"],\\\n",
    "                   get_filename_from_cd(r.headers.get('content-disposition')))\n",
    "        filenames.append(filename)\n",
    "    HAPI_dict['filenames'] = filenames\n",
    "    return HAPI_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:left;\">This project is licensed under the <a href=\"./LICENSE\">MIT License</a> <span style=\"float:right;\"><a href=\"https://gitlab.eumetsat.int/eo-lab/training-atmospheric-composition/\">View on GitLab</a> | <a href=\"https://training.eumetsat.int/\">EUMETSAT Training</a> | <a href=mailto:training@eumetsat.int>Contact</a></span></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
